{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "R-squared (also known as the coefficient of determination) is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in a linear regression model.\n",
    "\n",
    "Where:\n",
    "\n",
    "SS residual ​is the sum of squared residuals (errors) between the actual and predicted values\n",
    "\n",
    "SS total is the total sum of squares, representing the variance of the dependent variable.\n",
    "\n",
    "Higher R-squared values suggest a better fit, though this is not always the case, especially in complex models.\n",
    "\n",
    "\n",
    "# Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "Adjusted R-squared is a modified version of R-squared that adjusts for the number of predictors in the model. It is used to penalize the inclusion of irrelevant variables.\n",
    "\n",
    "Where:\n",
    "n is the number of data points.\n",
    "\n",
    "p is the number of predictors (independent variables).\n",
    "\n",
    "Difference from R-squared:\n",
    "\n",
    "Regular R-squared will always increase when adding more predictors to the model, even if the new predictors are irrelevant.\n",
    "\n",
    "Adjusted R-squared penalizes the inclusion of unnecessary predictors and can decrease if adding a predictor does not improve the model.\n",
    "\n",
    "\n",
    "# Q3. When is it more appropriate to use adjusted R-squared?\n",
    "Adjusted R-squared is more appropriate than regular R-squared when:\n",
    "\n",
    "You have multiple predictors (independent variables).\n",
    "You are comparing models with different numbers of predictors.\n",
    "You want to account for the complexity of the model (to prevent overfitting by including too many predictors).\n",
    "It provides a better understanding of the model’s true predictive power, especially when dealing with large datasets and multiple variables.\n",
    "\n",
    "# Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "MSE (Mean Squared Error): Measures the average squared difference between the actual and predicted values.\n",
    "\n",
    "RMSE (Root Mean Squared Error): The square root of the MSE. It provides the error in the same units as the target variable.\n",
    "\n",
    "MAE (Mean Absolute Error): Measures the average of the absolute differences between the actual and predicted values.\n",
    "\n",
    "# Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "MSE and RMSE:\n",
    "\n",
    "Advantages: More sensitive to large errors, providing a clear indication when large deviations exist in the predictions.\n",
    "Disadvantages: The squaring of errors can disproportionately penalize outliers, which may not be desirable in some cases.\n",
    "MAE:\n",
    "\n",
    "Advantages: Less sensitive to outliers than MSE/RMSE, providing a more straightforward interpretation of error magnitude.\n",
    "Disadvantages: It doesn’t differentiate the severity of errors as well as MSE or RMSE, so large errors are not penalized as heavily.\n",
    "\n",
    "# Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) is a form of regularization that adds a penalty to the absolute values of the coefficients, encouraging sparse models (many coefficients become zero)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
